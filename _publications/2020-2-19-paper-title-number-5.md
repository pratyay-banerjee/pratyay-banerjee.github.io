---
title: "VQA-LOL: Visual Question Answering under the Lens of Logic"
collection: publications
permalink: /publication/2020-2-19-paper-title-number-5
excerpt: 'In this paper I explore ways to make VQA models robust to Logical Questions'
date: 2020-2-19
venue: 'arxiv.org'
paperurl: 'https://arxiv.org/abs/2002.08325'
citation: 'Tejas Gokhale, Pratyay Banerjee, Chitta Baral and Yezhou Yang (2020, February). VQA-LOL: Visual Question Answering under the Lens of Logic. 16th European Conference on Computer Vision (ECCV 2020)'
---

Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate visual question answering (VQA) through the lens of logical transformation and posit that systems that seek to answer questions about images must be robust to these transformations of the question. If a VQA system is able to answer a question, it should also be able to answer the logical composition of questions. We analyze the performance of state-of-the-art models on the VQA task under these logical operations and show that they have difficulty in correctly answering such questions. We then construct an augmentation of the VQA dataset with questions containing logical operations and retrain the same models to establish a baseline. We further propose a novel methodology to train models to learn negation, conjunction, and disjunction and show improvement in learning logical composition and retaining performance on VQA. We suggest this work as a move towards embedding logical connectives in visual understanding, along with the benefits of robustness and generalizability. Our code and dataset is available online at this https URL 

[Download paper here](https://arxiv.org/abs/2002.08325)

Recommended citation: Tejas Gokhale, Pratyay Banerjee, Chitta Baral and Yezhou Yang (2020, February). VQA-LOL: Visual Question Answering under the Lens of Logic. 16th European Conference on Computer Vision (ECCV 2020). 